{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dining Predictor.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN0Gy7inYgR/NKJEEidTJCB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9G-59_A6-dl"
      },
      "source": [
        "# Setup plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "# Set Matplotlib defaults\n",
        "plt.rc('figure', autolayout=True)\n",
        "plt.rc('axes', labelweight='bold', labelsize='large',\n",
        "       titleweight='bold', titlesize=18, titlepad=10)\n",
        "plt.rc('animation', html='html5')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "# Holding the dataset filepaths\n",
        "line_file_path = 'temp'\n",
        "\n",
        "# Reading the data\n",
        "line_data = pd.read_csv(line_file_path)\n",
        "\n",
        "# Making numpy arrays out of the datasets\n",
        "line_numpy = np.array(line_data)\n",
        "\n",
        "# Transposing the numpy arrays\n",
        "line_numpy = line_numpy.T\n",
        "design_numpy = design_numpy.T\n",
        "\n",
        "# Splitting the arrays into training, validation, and test sets. 20%, 10%, 70% split\n",
        "line_test, line_train = train_test_split(line_numpy, test_size = 0.2, train_size = 0.8) #can I just add shuffle = true to this?\n",
        "line_valid, line_train = train_test_split(line_train, test_size = 0.125, train_size = 0.875)\n",
        "\n",
        "# Standardizing the data\n",
        "# find the standard deviation of each feature within the training set\n",
        "sigma = np.std(line_train, axis=0)\n",
        "# find the mean of each feature within the training set\n",
        "mu = np.mean(line_train, axis=0)\n",
        "# standardize our data based on mu and sigma from the training set\n",
        "standard_line_train = (line_train - mu) / sigma\n",
        "standard_line_valid = (line_valid - mu) / sigma\n",
        "standard_line_test = (line_test - mu) / sigma\n",
        "\n",
        "# Creating the neural network\n",
        "input_shape = [standard_line_train.shape[1]]\n",
        "model = keras.Sequential([\n",
        "    layers.BatchNormalization(input_shape=input_shape),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mae',\n",
        ")\n",
        "history = model.fit(\n",
        "    standard_line_train,\n",
        "    validation_data=(standard_line_valid),\n",
        "    batch_size=512,\n",
        "    epochs=150,\n",
        "    verbose=0,\n",
        ")\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.loc[0:, ['loss', 'val_loss']].plot()\n",
        "print(\"Minimum Absolute Error: {:0.4f}\".format(history_df['val_loss'].min()))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}